#!/usr/bin/env python3
"""
Model Quantization Tool
=======================

Convert trained PyTorch models to quantized format for MCU deployment.
"""

import torch
import torch.nn as nn
import numpy as np
from pathlib import Path


class QuantizedLSTM(nn.Module):
    """Quantized LSTM for deployment"""

    def __init__(self, float_model):
        super().__init__()
        self.hidden_dim = 32
        self.input_dim = 1
        self.seq_len = 10

        # Copy weights and convert to int8
        self.weight_ih = nn.Parameter(torch.zeros(4 * self.hidden_dim, self.input_dim).int())
        self.weight_hh = nn.Parameter(torch.zeros(4 * self.hidden_dim, self.hidden_dim).int())

        # Convert from float model
        if hasattr(float_model, 'lstm'):
            lstm = float_model.lstm
            self.weight_ih.data = lstm.weight_ih_l0.data.to(torch.int8)
            self.weight_hh.data = lstm.weight_hh_l0.data.to(torch.int8)

    def forward(self, x):
        """Quantized forward pass"""
        batch_size, seq_len, input_dim = x.shape
        h0 = torch.zeros(1, batch_size, self.hidden_dim).int()
        c0 = torch.zeros(1, batch_size, self.hidden_dim).int()

        h_t, c_t = h0, c0

        for t in range(seq_len):
            # LSTM gate computation (simplified)
            x_t = x[:, t, :].int()

            # Compute gates
            i_gate = torch.sigmoid(torch.matmul(x_t, self.weight_ih[:self.hidden_dim].t()))
            f_gate = torch.sigmoid(torch.matmul(x_t, self.weight_ih[self.hidden_dim:2*self.hidden_dim].t()))
            g_gate = torch.tanh(torch.matmul(x_t, self.weight_ih[2*self.hidden_dim:3*self.hidden_dim].t()))
            o_gate = torch.sigmoid(torch.matmul(x_t, self.weight_ih[3*self.hidden_dim:].t()))

            # Update cell and hidden
            c_t = f_gate * c_t + i_gate * g_gate
            h_t = o_gate * torch.tanh(c_t)

        return h_t.float()


def quantize_model(model: nn.Module, calibration_data: torch.Tensor) -> nn.Module:
    """
    Apply dynamic quantization to model

    Args:
        model: Float32 PyTorch model
        calibration_data: Sample data for quantization

    Returns:
        Quantized model
    """
    model.eval()

    # Apply dynamic quantization to LSTM and Linear layers
    quantized_model = torch.quantization.quantize_dynamic(
        model,
        {nn.LSTM, nn.Linear},
        dtype=torch.qint8
    )

    return quantized_model


def export_to_tflite(model: nn.Module, input_shape: tuple, output_path: str):
    """
    Export PyTorch model to TensorFlow Lite

    Args:
        model: PyTorch model
        input_shape: Input tensor shape
        output_path: Output .tflite path
    """
    try:
        import torch_to_tf_converter as tfc
    except ImportError:
        print("Installing torch2trt...")
        import subprocess
        subprocess.check_call(['pip', 'install', 'torch2trt'])
        import torch_to_tf_converter as tfc

    # Convert to ONNX first
    dummy_input = torch.randn(input_shape)
    torch.onnx.export(
        model,
        dummy_input,
        'models/tinylstm.onnx',
        export_params=True,
        opset_version=11,
        input_names=['input'],
        output_names=['output']
    )

    print(f"Model exported to ONNX: models/tinylstm.onnx")
    print(f"Use onnx-tflite converter to generate TFLite file")


def generate_c_weights(model: nn.Module, output_file: str):
    """
    Generate C array from quantized model

    Args:
        model: Quantized PyTorch model
        output_file: Output C header file path
    """
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w') as f:
        f.write("/* TinyLSTM Quantized Weights */\n")
        f.write("/* Auto-generated by model_quantization.py */\n\n")

        f.write("#ifndef TINYLSTM_WEIGHTS_H\n")
        f.write("#define TINYLSTM_WEIGHTS_H\n\n")

        f.write("#include <stdint.h>\n\n")

        # Extract weights
        if hasattr(model, 'lstm'):
            lstm = model.lstm

            # LSTM weights
            weight_ih = lstm.weight_ih_l0.data.numpy().astype(np.int8)
            weight_hh = lstm.weight_hh_l0.data.numpy().astype(np.int8)

            # Write weights as C arrays
            f.write(f"const int8_t LSTM_WEIGHT_IH[{weight_ih.shape[0] * weight_ih.shape[1]}] = {{\n")
            for i, val in enumerate(weight_ih.flatten()):
                f.write(f"  {val},")
                if (i + 1) % 16 == 0:
                    f.write("\n")
            f.write("\n};\n\n")

            f.write(f"const int8_t LSTM_WEIGHT_HH[{weight_hh.shape[0] * weight_hh.shape[1]}] = {{\n")
            for i, val in enumerate(weight_hh.flatten()):
                f.write(f"  {val},")
                if (i + 1) % 16 == 0:
                    f.write("\n")
            f.write("\n};\n\n")

            # Write metadata
            f.write(f"#define LSTM_HIDDEN_DIM {model.hidden_dim}\n")
            f.write(f"#define LSTM_INPUT_DIM {model.input_dim}\n")
            f.write(f"#define LSTM_SEQ_LEN {model.seq_len}\n")

        f.write("\n#endif /* TINYLSTM_WEIGHTS_H */\n")

    print(f"C weights exported to {output_path}")


def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description='Quantize ECTC models')
    parser.add_argument('--input', type=str, required=True,
                       help='Input PyTorch model file')
    parser.add_argument('--output', type=str, default='models/tinylstm_quantized.pt',
                       help='Output quantized model file')
    parser.add_argument('--export-c', action='store_true',
                       help='Export weights to C arrays')
    parser.add_argument('--export-tflite', action='store_true',
                       help='Export to TensorFlow Lite format')

    args = parser.parse_args()

    # Load model
    model = torch.load(args.input)
    print(f"Loaded model from {args.input}")

    # Create dummy calibration data
    calibration_data = torch.randn(100, 10, 1)

    # Quantize
    quantized_model = quantize_model(model, calibration_data)
    print("Model quantized")

    # Save quantized model
    torch.save(quantized_model, args.output)
    print(f"Quantized model saved to {args.output}")

    # Export to C
    if args.export_c:
        generate_c_weights(quantized_model, 'firmware/ectc_node/include/tinylstm_weights.h')

    # Export to TFLite
    if args.export_tflite:
        export_to_tflite(quantized_model, (1, 10, 1), 'gateway/ectc_gateway/models/tinylstm_int8.tflite')


if __name__ == '__main__':
    main()
